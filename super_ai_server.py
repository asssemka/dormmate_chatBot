from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import logging
from typing import Optional
import time
import re
from perfect_database import PerfectDatabase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="üéØ –ò–î–ï–ê–õ–¨–ù–û –¢–û–ß–ù–´–ô AI –ß–∞—Ç–ë–æ—Ç", version="7.0.0")


class ChatRequest(BaseModel):
    question: str
    user_id: Optional[str] = None
    max_tokens: int = 100


class ChatResponse(BaseModel):
    model_config = ConfigDict(protected_namespaces=())

    answer: str
    confidence: float
    model_info: str
    generation_time: float


class PerfectChatbot:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.database = PerfectDatabase()
        self.load_model()

    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        try:
            model_path = "./fixed_super_model"
            base_model_name = "ai-forever/rugpt3small_based_on_gpt2"

            print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {self.device}")

            import os
            if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "adapter_config.json")):
                print("üéØ –ó–∞–≥—Ä—É–∂–∞–µ–º –¢–û–ß–ù–£–Æ –ú–û–î–ï–õ–¨...")

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )

                # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
                self.model = PeftModel.from_pretrained(base_model, model_path)
                self.model = self.model.to(self.device)

                print("‚úÖ –¢–û–ß–ù–ê–Ø –ú–û–î–ï–õ–¨ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

            else:
                print("‚ö†Ô∏è –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é...")

                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                self.model = self.model.to(self.device)

                print("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.model.eval()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            logger.info("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Ç–æ–ª—å–∫–æ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö")

    def clean_response(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        artifacts = [
            r'<\|.*?\|>', r'<.*?>', r'\|.*?\|',
            r'<endoftext>', r'<startoftext>', r'<pad>',
            r'endof.*?', r'enabled.*?', r'Beta.*?',
            r'Factory.*?', r'Container.*?', r'\[.*?\]'
        ]

        for artifact in artifacts:
            text = re.sub(artifact, '', text, flags=re.IGNORECASE)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^\w\s.,!?:;‚Ññ()-]', '', text)
        text = re.sub(r'\s+', ' ', text)

        return text.strip()

    def generate_model_response(self, question: str, max_tokens: int = 100) -> tuple:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏"""
        if self.model is None or self.tokenizer is None:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö.", 0.0, 0.0

        start_time = time.time()

        try:
            # –ü—Ä–æ–º–ø—Ç
            prompt = f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=200
            ).to(self.device)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.3,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            if "–û—Ç–≤–µ—Ç:" in response:
                answer = response.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            else:
                answer = response.replace(prompt, "").strip()

            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            answer = self.clean_response(answer)

            generation_time = time.time() - start_time

            return answer, 0.5, generation_time

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", 0.1, 0.0

    def get_perfect_answer(self, question: str, max_tokens: int = 100) -> tuple:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        start_time = time.time()

        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        db_answer, confidence = self.database.get_answer(question)

        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        if confidence >= 0.7:
            generation_time = time.time() - start_time
            return db_answer, confidence, generation_time

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å
        model_answer, model_confidence, model_time = self.generate_model_response(question, max_tokens)

        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –ª—É—á—à–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if confidence > model_confidence:
            generation_time = time.time() - start_time
            return db_answer, confidence, generation_time

        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        return model_answer, model_confidence, model_time


# –°–æ–∑–¥–∞–µ–º –±–æ—Ç–∞
chatbot = PerfectChatbot()


@app.get("/")
def root():
    import os
    model_type = "–ò–î–ï–ê–õ–¨–ù–û –¢–û–ß–ù–ê–Ø –ú–û–î–ï–õ–¨" if os.path.exists("./fixed_super_model") else "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö + –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"
    return {
        "message": "üéØ –ò–î–ï–ê–õ–¨–ù–û –¢–û–ß–ù–´–ô AI –ß–∞—Ç–ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "model_info": model_type,
        "device": str(chatbot.device),
        "university": "–ê–ª–º–∞—Ç—ã",
        "database_size": len(chatbot.database.qa_pairs),
        "status": "ready"
    }


@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –∏–¥–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"""
    try:
        logger.info(f"üìù –í–æ–ø—Ä–æ—Å –æ—Ç {request.user_id}: {request.question}")

        answer, confidence, gen_time = chatbot.get_perfect_answer(
            request.question,
            request.max_tokens
        )

        import os
        model_info = "perfect_hybrid_model" if os.path.exists("./fixed_super_model") else "perfect_database"

        return ChatResponse(
            answer=answer,
            confidence=confidence,
            model_info=model_info,
            generation_time=gen_time
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
