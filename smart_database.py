"""
–£–º–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞
"""
import json
import re
from typing import Dict, List, Tuple, Optional
from difflib import SequenceMatcher


class SmartDatabase:
    def __init__(self, data_path: str = "dataset.jsonl"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–º–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        self.qa_pairs = []
        self.synonyms = self._create_synonyms()
        self.slang_mapping = self._create_slang_mapping()
        self.stop_phrases = self._create_stop_phrases()
        self.load_data(data_path)
        self.create_smart_index()

    def _create_synonyms(self) -> Dict[str, List[str]]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
        return {
            '–æ–±—â–µ–∂–∏—Ç–∏–µ': [
                '–æ–±—â–∞–≥–∞', '–æ–±—â–∞–∫', '–æ–±—â–µ–∂–∏—Ç–∏—è', '–æ–±—â–∞–≥–∏', '–æ–±—â–∞–≥–∞—Ö', '–æ–±—â–∞–≥–µ',
                '–¥–æ–º —Å—Ç—É–¥–µ–Ω—Ç–æ–≤', '–¥—Å', '—Å—Ç—É–¥–¥–æ–º', '—Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–π –¥–æ–º',
                '–æ–±—â–µ–∂–∏—Ç–∏–π', '–æ–±—â–µ–∂–∏—Ç–∏–µ', '–æ–±—â–µ–∂–∏—Ç–∏–µ–º', '–æ–±—â–µ–∂–∏—Ç–∏–∏',
                '—Å—Ç—É–¥–≥–æ—Ä–æ–¥–æ–∫', '—Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–µ –æ–±—â–µ–∂–∏—Ç–∏–µ', '—Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è –æ–±—â–∞–≥–∞',
                '–∂–∏–ª—å–µ', '–∂–∏–ª—å—ë', '–º–µ—Å—Ç–æ', '–∫–æ–º–Ω–∞—Ç–∞', '–∫–æ–π–∫–∞', '–∫–æ–π–∫–æ-–º–µ—Å—Ç–æ'
            ],
            '–≥–¥–µ': ['–∫—É–¥–∞', '–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–∏—Å–∫–∞—Ç—å', '–Ω–∞–π—Ç–∏'],
            '–∫–∞–∫': ['–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '—Å–ø–æ—Å–æ–±', '–º–µ—Ç–æ–¥', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞'],
            '–ø–æ–¥–∞—Ç—å': ['–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–ø–æ—Å–ª–∞—Ç—å', '—Å–¥–∞—Ç—å', '–ø–µ—Ä–µ–¥–∞—Ç—å', '–ø–æ–¥–∞–≤–∞—Ç—å', '–æ—Ñ–æ—Ä–º–∏—Ç—å'],
            '–∑–∞—è–≤–∫—É': ['–∑–∞—è–≤–ª–µ–Ω–∏–µ', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '–±—É–º–∞–≥–∏', '–∞–Ω–∫–µ—Ç—É', '—Ñ–æ—Ä–º—É'],
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã': ['—Å–ø—Ä–∞–≤–∫–∏', '–±—É–º–∞–≥–∏', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', '–±—É–º–∞–∂–∫–∏', '—Å–ø—Ä–∞–≤–æ—á–∫–∏'],
            '–æ–ø–ª–∞—Ç–∏—Ç—å': ['–∑–∞–ø–ª–∞—Ç–∏—Ç—å', '–≤–Ω–µ—Å—Ç–∏ –ø–ª–∞—Ç—É', '—Ä–∞—Å–ø–ª–∞—Ç–∏—Ç—å—Å—è', '–ø–ª–∞—Ç–∏—Ç—å', '–æ–ø–ª–∞—á–∏–≤–∞—Ç—å'],
            '–æ—Ç–∫–∞–∑–∞–ª–∏': ['–Ω–µ –¥–∞–ª–∏', '–æ—Ç–∫–ª–æ–Ω–∏–ª–∏', '–Ω–µ –æ–¥–æ–±—Ä–∏–ª–∏', '–Ω–µ –ø—Ä–æ—à–µ–ª', '–Ω–µ –ø—Ä–æ—à–ª–∞', '–æ—Ç–∫–∞–∑'],
            '–∞–ø–µ–ª–ª—è—Ü–∏—è': ['–∂–∞–ª–æ–±–∞', '–ø–µ—Ä–µ—Å–º–æ—Ç—Ä', '–æ–±–∂–∞–ª–æ–≤–∞–Ω–∏–µ', '–ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–¥–∞—á–∞'],
            '–∫–æ–º–Ω–∞—Ç–∞': ['–∫–æ–º–Ω–∞—Ç–∫–∞', '–ø–æ–º–µ—â–µ–Ω–∏–µ', '–Ω–æ–º–µ—Ä', '–±–ª–æ–∫', '—Å–µ–∫—Ü–∏—è'],
            '—á–µ–ª–æ–≤–µ–∫': ['–ª—é–¥–µ–π', '—Å—Ç—É–¥–µ–Ω—Ç–æ–≤', '–∂–∏–ª—å—Ü–æ–≤', '–Ω–∞—Ä–æ–¥—É', '—á–µ–ª'],
            '–º–æ–∂–Ω–æ': ['—Ä–∞–∑—Ä–µ—à–µ–Ω–æ', '–ø–æ–∑–≤–æ–ª–µ–Ω–æ', '–¥–æ–ø—É—Å—Ç–∏–º–æ', '–≤–æ–∑–º–æ–∂–Ω–æ'],
            '–Ω–µ–ª—å–∑—è': ['–∑–∞–ø—Ä–µ—â–µ–Ω–æ', '–Ω–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ', '–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–æ', '–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ']
        }

    def _create_slang_mapping(self) -> Dict[str, str]:
        """–°–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
        return {
            # –ë–∞–∑–æ–≤—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
            '—á–µ': '—á—Ç–æ',
            '—á—ë': '—á—Ç–æ',
            '—á–æ': '—á—Ç–æ',
            '—à–æ': '—á—Ç–æ',
            '–∫—Ç–æ': '–∫—Ç–æ',
            '–∫–∞–¥–∞': '–∫–æ–≥–¥–∞',
            '—â–∞—Å': '—Å–µ–π—á–∞—Å',
            '—Ç–æ–∫–∞': '—Ç–æ–ª—å–∫–æ',
            '—Ç–æ–∫–æ': '—Ç–æ–ª—å–∫–æ',
            '–Ω–∞–¥–∞': '–Ω–∞–¥–æ',
            '–Ω–∞–¥–æ': '–Ω—É–∂–Ω–æ',
            '–∫–∞–Ω–µ—à': '–∫–æ–Ω–µ—á–Ω–æ',
            '–∫–∞–Ω–µ—à–Ω': '–∫–æ–Ω–µ—á–Ω–æ',
            '–Ω–æ—Ä–º': '–Ω–æ—Ä–º–∞–ª—å–Ω–æ',
            '–æ–∫–µ–π': '—Ö–æ—Ä–æ—à–æ',
            '–æ–∫': '—Ö–æ—Ä–æ—à–æ',
            '—Å–ø—Å': '—Å–ø–∞—Å–∏–±–æ',
            '–ø–∂–ª': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
            '–ø–ª–∑': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
            '–∏–Ω—Ñ–∞': '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            '–∏–Ω—Ñ–æ': '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',

            # –°–æ–∫—Ä–∞—â–µ–Ω–∏—è –æ–±—â–µ–∂–∏—Ç–∏–π
            '–¥—Å3': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å3',
            '–¥—Å2–∞': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å2–∞',
            '–¥—Å2–±': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å2–±',
            '–¥—Å-3': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å3',
            '–¥—Å-2–∞': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å2–∞',
            '–¥—Å-2–±': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å2–±',
            '–¥—Å 3': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å3',
            '–¥—Å 2–∞': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å2–∞',
            '–¥—Å 2–±': '–æ–±—â–µ–∂–∏—Ç–∏–µ –¥—Å2–±',

            # –í–∞—Ä–∏–∞–Ω—Ç—ã —Å–ª–æ–≤–∞ "–æ–±—â–µ–∂–∏—Ç–∏–µ"
            '–æ–±—â–∞–≥–∞': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–∫': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–≥–∏': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–≥–∞—Ö': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–≥–µ': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–≥—É': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–≥–æ–π': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–∞–≥': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–µ–∂–∏—Ç–∏—è': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–µ–∂–∏—Ç–∏–π': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–µ–∂–∏—Ç–∏–µ–º': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–æ–±—â–µ–∂–∏—Ç–∏–∏': '–æ–±—â–µ–∂–∏—Ç–∏–µ',

            # –î–æ–º —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
            '–¥—Å': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '—Å—Ç—É–¥–¥–æ–º': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '—Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–π –¥–æ–º': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–¥–æ–º —Å—Ç—É–¥–µ–Ω—Ç–æ–≤': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '—Å—Ç—É–¥–≥–æ—Ä–æ–¥–æ–∫': '–æ–±—â–µ–∂–∏—Ç–∏–µ',

            # –î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            '–∂–∏–ª—å–µ': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–∂–∏–ª—å—ë': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–º–µ—Å—Ç–æ': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–∫–æ–π–∫–∞': '–æ–±—â–µ–∂–∏—Ç–∏–µ',
            '–∫–æ–π–∫–æ-–º–µ—Å—Ç–æ': '–æ–±—â–µ–∂–∏—Ç–∏–µ'
        }

    def _create_stop_phrases(self) -> List[str]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        return [
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.",
            "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —É –º–µ–Ω—è –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.",
            "–≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –º–æ–∏—Ö –∑–Ω–∞–Ω–∏–π.",
            "–Ø –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.",
            "–ü–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –ª—É—á—à–µ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏.",
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ —è –Ω–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é —Ç–∞–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."
        ]

    def load_data(self, data_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSONL —Ñ–∞–π–ª–∞"""
        print(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º —É–º–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_path}...")

        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line.strip())
                        self.qa_pairs.append({
                            'question': item['instruction'],
                            'answer': item['output'],
                            'keywords': self._extract_smart_keywords(item['instruction']),
                            'normalized_question': self._normalize_text(item['instruction'])
                        })

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.qa_pairs)} —É–º–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            self._create_fallback_data()

    def _create_fallback_data(self):
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö"""
        fallback_data = [
            {
                'question': '–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°3?',
                'answer': '–û–±—â–µ–∂–∏—Ç–∏–µ ‚Ññ3 –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –º–∫—Ä ‚Ññ1 81–ê.',
            },
            {
                'question': '–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°2–±?',
                'answer': '–û–±—â–µ–∂–∏—Ç–∏–µ –î–°2–± –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –¢–∞—É–≥—É–ª—å 34.',
            },
            {
                'question': '–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°2–∞?',
                'answer': '–û–±—â–µ–∂–∏—Ç–∏–µ –î–°2–ê –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –¢–∞—É–≥—É–ª—å 32.',
            },
            {
                'question': '–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –ï–º–µ–Ω?',
                'answer': '–û–±—â–µ–∂–∏—Ç–∏–µ –ï–º–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –º–∫—Ä ‚Ññ10 26/1.',
            },
            {
                'question': '–ú–Ω–µ –Ω–µ –¥–∞–ª–∏ –æ–±—â–µ–∂–∏—Ç–∏–µ',
                'answer': '–†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –¥–µ–∫–∞–Ω–∞—Ç –∏–ª–∏ –æ—Ç–¥–µ–ª —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –∑–∞—è–≤–∫–∏.',
            }
        ]

        self.qa_pairs = []
        for item in fallback_data:
            self.qa_pairs.append({
                'question': item['question'],
                'answer': item['answer'],
                'keywords': self._extract_smart_keywords(item['question']),
                'normalized_question': self._normalize_text(item['question'])
            })

        print("‚ö†Ô∏è –°–æ–∑–¥–∞–Ω –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö")

    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç (—É–±–∏—Ä–∞–µ—Ç —Å–ª–µ–Ω–≥, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É)"""
        text = text.lower().strip()

        # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–µ–Ω–≥
        words = text.split()
        normalized_words = []

        for word in words:
            # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            clean_word = re.sub(r'[^\w]', '', word)

            # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–µ–Ω–≥
            if clean_word in self.slang_mapping:
                normalized_words.append(self.slang_mapping[clean_word])
            else:
                normalized_words.append(clean_word)

        return ' '.join(normalized_words)

    def _extract_smart_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–º–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å —É—á–µ—Ç–æ–º —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
        normalized_text = self._normalize_text(text)

        # –ë–∞–∑–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∑–∞', '–æ—Ç', '–∫', '—É', '–æ', '–∏–∑', '–∞', '–Ω–æ', '–∏–ª–∏', '—ç—Ç–æ', '—Ç–æ',
                      '–∫–∞–∫', '—á—Ç–æ'}
        words = [word for word in normalized_text.split() if word not in stop_words and len(word) > 1]

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        extended_keywords = set(words)
        for word in words:
            for main_word, synonyms in self.synonyms.items():
                if word in synonyms or word == main_word:
                    extended_keywords.add(main_word)
                    extended_keywords.update(synonyms)

        return list(extended_keywords)

    def create_smart_index(self):
        """–°–æ–∑–¥–∞–µ—Ç —É–º–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        self.keyword_index = {}
        self.question_similarity = {}

        for i, qa_pair in enumerate(self.qa_pairs):
            # –ò–Ω–¥–µ–∫—Å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            for keyword in qa_pair['keywords']:
                if keyword not in self.keyword_index:
                    self.keyword_index[keyword] = []
                self.keyword_index[keyword].append(i)

            # –ò–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
            self.question_similarity[i] = qa_pair['normalized_question']

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —É–º–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏–∑ {len(self.keyword_index)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
        return SequenceMatcher(None, text1, text2).ratio()

    def _get_stop_phrase(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—É"""
        import random
        return random.choice(self.stop_phrases)

    def find_smart_match(self, question: str) -> Tuple[str, float]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–º–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞"""
        normalized_question = self._normalize_text(question)

        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for qa_pair in self.qa_pairs:
            if qa_pair['normalized_question'] == normalized_question:
                return qa_pair['answer'], 1.0

        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±—â–µ–∂–∏—Ç–∏–π
        answer, confidence = self._check_dormitory_patterns(normalized_question)
        if confidence > 0.8:
            return answer, confidence

        # 3. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keywords = self._extract_smart_keywords(question)
        if not keywords:
            return self._get_stop_phrase(), 0.1

        # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        scores = []
        for i, qa_pair in enumerate(self.qa_pairs):
            matched_keywords = set(keywords) & set(qa_pair['keywords'])
            keyword_score = len(matched_keywords) / max(len(keywords), len(qa_pair['keywords']))

            # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–Ω—É—Å –∑–∞ —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            similarity_score = self._calculate_similarity(normalized_question, qa_pair['normalized_question'])

            # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
            total_score = (keyword_score * 0.7) + (similarity_score * 0.3)

            # –ë–æ–Ω—É—Å –∑–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            special_keywords = ['–¥—Å3', '–¥—Å2–∞', '–¥—Å2–±', '–µ–º–µ–Ω', '–æ–±—â–µ–∂–∏—Ç–∏–µ']
            for keyword in special_keywords:
                if keyword in matched_keywords:
                    total_score += 0.2

            scores.append((i, total_score))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–∫–∏
        scores.sort(key=lambda x: x[1], reverse=True)

        # –ï—Å–ª–∏ –ª—É—á—à–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç
        if scores and scores[0][1] > 0.4:
            best_match_idx = scores[0][0]
            return self.qa_pairs[best_match_idx]['answer'], scores[0][1]

        # –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—É
        return self._get_stop_phrase(), 0.1

    def _check_dormitory_patterns(self, normalized_question: str) -> Tuple[str, float]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±—â–µ–∂–∏—Ç–∏–π"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –î–°3
        ds3_patterns = ['–¥—Å3', '–¥—Å 3', '–¥—Å-3', '–æ–±—â–µ–∂–∏—Ç–∏–µ 3', '—Ç—Ä–µ—Ç—å–µ –æ–±—â–µ–∂–∏—Ç–∏–µ', '—Ç—Ä–µ—Ç—å—è –æ–±—â–∞–≥–∞', '—Ç—Ä–µ—Ç–∏–π –¥—Å']
        if any(pattern in normalized_question for pattern in ds3_patterns):
            if any(word in normalized_question for word in
                   ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–∫—É–¥–∞', '–∏—Å–∫–∞—Ç—å', '–Ω–∞–π—Ç–∏']):
                return '–û–±—â–µ–∂–∏—Ç–∏–µ ‚Ññ3 –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –º–∫—Ä ‚Ññ1 81–ê.', 0.95

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –î–°2–∞
        ds2a_patterns = ['–¥—Å2–∞', '–¥—Å 2–∞', '–¥—Å-2–∞', '–æ–±—â–µ–∂–∏—Ç–∏–µ 2–∞', '–≤—Ç–æ—Ä–∞—è –∞ –æ–±—â–∞–≥–∞', '–¥—Å2 –∞']
        if any(pattern in normalized_question for pattern in ds2a_patterns):
            if any(word in normalized_question for word in
                   ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–∫—É–¥–∞', '–∏—Å–∫–∞—Ç—å', '–Ω–∞–π—Ç–∏']):
                return '–û–±—â–µ–∂–∏—Ç–∏–µ –î–°2–ê –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –¢–∞—É–≥—É–ª—å 32.', 0.95

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –î–°2–±
        ds2b_patterns = ['–¥—Å2–±', '–¥—Å 2–±', '–¥—Å-2–±', '–æ–±—â–µ–∂–∏—Ç–∏–µ 2–±', '–≤—Ç–æ—Ä–∞—è –± –æ–±—â–∞–≥–∞', '–¥—Å2 –±']
        if any(pattern in normalized_question for pattern in ds2b_patterns):
            if any(word in normalized_question for word in
                   ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–∫—É–¥–∞', '–∏—Å–∫–∞—Ç—å', '–Ω–∞–π—Ç–∏']):
                return '–û–±—â–µ–∂–∏—Ç–∏–µ –î–°2–± –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –¢–∞—É–≥—É–ª—å 34.', 0.95

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ï–º–µ–Ω
        emen_patterns = ['–µ–º–µ–Ω', '–æ–±—â–µ–∂–∏—Ç–∏–µ –µ–º–µ–Ω', '–æ–±—â–∞–≥–∞ –µ–º–µ–Ω']
        if any(pattern in normalized_question for pattern in emen_patterns):
            if any(word in normalized_question for word in
                   ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω', '–∫—É–¥–∞', '–∏—Å–∫–∞—Ç—å', '–Ω–∞–π—Ç–∏']):
                return '–û–±—â–µ–∂–∏—Ç–∏–µ –ï–º–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –º–∫—Ä ‚Ññ10 26/1.', 0.95

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—Ç–∫–∞–∑–∞ –≤ –æ–±—â–µ–∂–∏—Ç–∏–∏
        rejection_patterns = ['–Ω–µ –¥–∞–ª–∏', '–æ—Ç–∫–∞–∑–∞–ª–∏', '–Ω–µ –æ–¥–æ–±—Ä–∏–ª–∏', '–Ω–µ –ø—Ä–æ—à–µ–ª', '–Ω–µ –ø—Ä–æ—à–ª–∞', '–æ—Ç–∫–∞–∑']
        housing_patterns = ['–æ–±—â–µ–∂–∏—Ç–∏–µ', '–æ–±—â–∞–≥–∞', '–æ–±—â–∞–∫', '–¥—Å', '–∂–∏–ª—å–µ', '–º–µ—Å—Ç–æ']
        if any(pattern in normalized_question for pattern in rejection_patterns):
            if any(pattern in normalized_question for pattern in housing_patterns):
                return '–†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –¥–µ–∫–∞–Ω–∞—Ç –∏–ª–∏ –æ—Ç–¥–µ–ª —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –∑–∞—è–≤–∫–∏.', 0.9

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏
        application_patterns = ['–∫–∞–∫ –ø–æ–¥–∞—Ç—å', '–ø–æ–¥–∞—á–∞ –∑–∞—è–≤–∫–∏', '–ø–æ–¥–∞—Ç—å –∑–∞—è–≤–ª–µ–Ω–∏–µ', '–æ—Ñ–æ—Ä–º–∏—Ç—å –∑–∞—è–≤–∫—É',
                                '–ø–æ–¥–∞–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã']
        if any(pattern in normalized_question for pattern in application_patterns):
            if any(pattern in normalized_question for pattern in housing_patterns):
                return '–ü–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É –º–æ–∂–Ω–æ —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç –Ω–∞ —Å–∞–π—Ç–µ, –∑–∞–ø–æ–ª–Ω–∏–≤ –∞–Ω–∫–µ—Ç—É –∏ –ø—Ä–∏–ª–æ–∂–∏–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.', 0.9

        # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ –æ–±—â–µ–∂–∏—Ç–∏—è—Ö
        general_housing_questions = [
            ('–∫–∞–∫–∏–µ –æ–±—â–∞–≥–∏',
             '–í —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ –µ—Å—Ç—å –æ–±—â–µ–∂–∏—Ç–∏—è: –î–°3 (–º–∫—Ä ‚Ññ1 81–ê), –î–°2–ê (–¢–∞—É–≥—É–ª—å 32), –î–°2–± (–¢–∞—É–≥—É–ª—å 34), –ï–º–µ–Ω (–º–∫—Ä ‚Ññ10 26/1).'),
            ('–∫–∞–∫–∏–µ –æ–±—â–µ–∂–∏—Ç–∏—è',
             '–í —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ –µ—Å—Ç—å –æ–±—â–µ–∂–∏—Ç–∏—è: –î–°3 (–º–∫—Ä ‚Ññ1 81–ê), –î–°2–ê (–¢–∞—É–≥—É–ª—å 32), –î–°2–± (–¢–∞—É–≥—É–ª—å 34), –ï–º–µ–Ω (–º–∫—Ä ‚Ññ10 26/1).'),
            ('—Å–ø–∏—Å–æ–∫ –æ–±—â–∞–≥',
             '–í —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ –µ—Å—Ç—å –æ–±—â–µ–∂–∏—Ç–∏—è: –î–°3 (–º–∫—Ä ‚Ññ1 81–ê), –î–°2–ê (–¢–∞—É–≥—É–ª—å 32), –î–°2–± (–¢–∞—É–≥—É–ª—å 34), –ï–º–µ–Ω (–º–∫—Ä ‚Ññ10 26/1).'),
            ('—Å–∫–æ–ª—å–∫–æ –æ–±—â–∞–≥', '–í —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ 4 –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–±—â–µ–∂–∏—Ç–∏—è: –î–°3, –î–°2–ê, –î–°2–± –∏ –ï–º–µ–Ω.'),
            ('–≤—Å–µ –æ–±—â–µ–∂–∏—Ç–∏—è',
             '–í —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ –µ—Å—Ç—å –æ–±—â–µ–∂–∏—Ç–∏—è: –î–°3 (–º–∫—Ä ‚Ññ1 81–ê), –î–°2–ê (–¢–∞—É–≥—É–ª—å 32), –î–°2–± (–¢–∞—É–≥—É–ª—å 34), –ï–º–µ–Ω (–º–∫—Ä ‚Ññ10 26/1).')
        ]

        for pattern, answer in general_housing_questions:
            if pattern in normalized_question:
                return answer, 0.9

        return "", 0.0

    def get_smart_answer(self, question: str) -> Tuple[str, float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —É–º–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—É—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å
        if not question.strip():
            return self._get_stop_phrase(), 0.1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å
        if len(question.strip()) < 3:
            return self._get_stop_phrase(), 0.1

        # –ò—â–µ–º —É–º–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        answer, confidence = self.find_smart_match(question)

        # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—É
        if confidence < 0.3:
            return self._get_stop_phrase(), 0.1

        return answer, confidence


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–º–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
if __name__ == "__main__":
    db = SmartDatabase()

    test_questions = [
        "–ì–¥–µ –î–°3?",
        "–ß–µ –∑–∞ –∞–¥—Ä–µ—Å —É –¥—Å2–∞?",
        "–ö—É–¥–∞ –µ—Ö–∞—Ç—å –µ—Å–ª–∏ –¥–∞–ª–∏ –º–µ—Å—Ç–æ –≤ –¥—Å3?",
        "–ú–Ω–µ –Ω–µ –¥–∞–ª–∏ –æ–±—â–∞–≥—É, —á–µ –¥–µ–ª–∞—Ç—å?",
        "–ö–∞–∫ –ø–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É?",
        "–ö–∞–∫–∏–µ –æ–±—â–∞–≥–∏ –µ—Å—Ç—å?",
        "–°–∫–æ–ª—å–∫–æ –º–µ—Å—Ç –≤ –∫–æ–º–Ω–∞—Ç–∞—Ö?",
        "–ß–µ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å —Å —Å–æ–±–æ–π?",
        "–ü—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞?",
        "–ì–¥–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞?",
        "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞?"
    ]

    print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–ú–ù–û–ô –ë–ê–ó–´ –î–ê–ù–ù–´–•:")
    for question in test_questions:
        answer, confidence = db.get_smart_answer(question)
        print(f"‚ùì {question}")
        print(f"ü§ñ {answer}")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
        print("-" * 50)