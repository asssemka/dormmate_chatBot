import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import json
import os


class SmartTrainer:
    def __init__(self):
        self.model_name = "ai-forever/rugpt3small_based_on_gpt2"
        self.output_dir = "./smart_model"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def create_enhanced_dataset(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
        print("üìö –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")

        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        base_data = []
        try:
            with open("dataset.jsonl", "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        item = json.loads(line.strip())
                        base_data.append(item)
        except:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä
            base_data = [
                {"instruction": "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°3?",
                 "output": "–û–±—â–µ–∂–∏—Ç–∏–µ ‚Ññ3 –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –º–∫—Ä ‚Ññ1 81–ê."},
                {"instruction": "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°2–∞?",
                 "output": "–û–±—â–µ–∂–∏—Ç–∏–µ –î–°2–ê –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –¢–∞—É–≥—É–ª—å 32."},
                {"instruction": "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°2–±?",
                 "output": "–û–±—â–µ–∂–∏—Ç–∏–µ –î–°2–± –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –¢–∞—É–≥—É–ª—å 34."},
                {"instruction": "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –ï–º–µ–Ω?",
                 "output": "–û–±—â–µ–∂–∏—Ç–∏–µ –ï–º–µ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ø–æ –∞–¥—Ä–µ—Å—É: –≥. –ê–ª–º–∞—Ç—ã, –º–∫—Ä ‚Ññ10 26/1."},
                {"instruction": "–ú–Ω–µ –Ω–µ –¥–∞–ª–∏ –æ–±—â–µ–∂–∏—Ç–∏–µ",
                 "output": "–†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –¥–µ–∫–∞–Ω–∞—Ç –∏–ª–∏ –æ—Ç–¥–µ–ª —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –∑–∞—è–≤–∫–∏."}
            ]

        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        enhanced_data = []

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        for item in base_data:
            enhanced_data.append(item)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        slang_variants = {
            "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°3?": [
                "–ì–¥–µ –î–°3?", "–ß–µ –∑–∞ –∞–¥—Ä–µ—Å —É –î–°3?", "–ö—É–¥–∞ –µ—Ö–∞—Ç—å –µ—Å–ª–∏ –¥–∞–ª–∏ –î–°3?",
                "–î–°3 –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è?", "–ê–¥—Ä–µ—Å –î–°3 –∫–∞–∫–æ–π?", "–ì–¥–µ —Ç—Ä–µ—Ç—å–µ –æ–±—â–µ–∂–∏—Ç–∏–µ?",
                "–ö—É–¥–∞ –∏–¥—Ç–∏ –µ—Å–ª–∏ –∑–∞—Å–µ–ª–∏–ª–∏ –≤ –î–°3?", "–ì–¥–µ —Ç—Ä–µ—Ç—å—è –æ–±—â–∞–≥–∞?",
                "–ê–¥—Ä–µ—Å —Ç—Ä–µ—Ç—å–µ–π –æ–±—â–∞–≥–∏?", "–ì–¥–µ –¥—Å 3?", "–ì–¥–µ –¥—Å-3?",
                "–ö—É–¥–∞ –µ—Ö–∞—Ç—å –≤ —Ç—Ä–µ—Ç—å—é –æ–±—â–∞–≥—É?", "–¢—Ä–µ—Ç–∏–π –¥—Å –≥–¥–µ?"
            ],
            "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°2–∞?": [
                "–ì–¥–µ –î–°2–∞?", "–ß–µ –∑–∞ –∞–¥—Ä–µ—Å —É –î–°2–∞?", "–î–°2–∞ –≥–¥–µ?",
                "–ê–¥—Ä–µ—Å –î–°2–∞?", "–ö—É–¥–∞ –µ—Ö–∞—Ç—å –≤ –î–°2–∞?", "–ì–¥–µ –¥—Å 2–∞?",
                "–ì–¥–µ –¥—Å-2–∞?", "–í—Ç–æ—Ä–∞—è –∞ –æ–±—â–∞–≥–∞ –≥–¥–µ?", "–ê–¥—Ä–µ—Å –≤—Ç–æ—Ä–æ–π –∞ –æ–±—â–∞–≥–∏?"
            ],
            "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –î–°2–±?": [
                "–ì–¥–µ –î–°2–±?", "–ß–µ –∑–∞ –∞–¥—Ä–µ—Å —É –î–°2–±?", "–î–°2–± –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è?",
                "–ê–¥—Ä–µ—Å –î–°2–± –∫–∞–∫–æ–π?", "–ì–¥–µ –¥—Å 2–±?", "–ì–¥–µ –¥—Å-2–±?",
                "–í—Ç–æ—Ä–∞—è –± –æ–±—â–∞–≥–∞ –≥–¥–µ?", "–ê–¥—Ä–µ—Å –≤—Ç–æ—Ä–æ–π –± –æ–±—â–∞–≥–∏?"
            ],
            "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ–±—â–µ–∂–∏—Ç–∏–µ –ï–º–µ–Ω?": [
                "–ì–¥–µ –ï–º–µ–Ω?", "–ï–º–µ–Ω –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è?", "–ê–¥—Ä–µ—Å –ï–º–µ–Ω–∞?",
                "–ö—É–¥–∞ –µ—Ö–∞—Ç—å –≤ –ï–º–µ–Ω?", "–û–±—â–∞–≥–∞ –ï–º–µ–Ω –≥–¥–µ?", "–ï–º–µ–Ω –∞–¥—Ä–µ—Å –∫–∞–∫–æ–π?"
            ],
            "–ú–Ω–µ –Ω–µ –¥–∞–ª–∏ –æ–±—â–µ–∂–∏—Ç–∏–µ": [
                "–ù–µ –¥–∞–ª–∏ –æ–±—â–∞–≥—É", "–û—Ç–∫–∞–∑–∞–ª–∏ –≤ –æ–±—â–µ–∂–∏—Ç–∏–∏", "–ù–µ –ø—Ä–æ—à–µ–ª –≤ –æ–±—â–∞–≥—É",
                "–ú–Ω–µ –æ—Ç–∫–∞–∑–∞–ª–∏ –≤ –æ–±—â–µ–∂–∏—Ç–∏–∏", "–ß–µ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –Ω–µ –¥–∞–ª–∏ –æ–±—â–∞–≥—É?",
                "–ù–µ –¥–∞–ª–∏ –º–µ—Å—Ç–æ –≤ –æ–±—â–∞–≥–µ", "–û—Ç–∫–∞–∑ –≤ –æ–±—â–µ–∂–∏—Ç–∏–∏", "–ù–µ –ø—Ä–æ—à–ª–∞ –≤ –æ–±—â–∞–≥—É",
                "–ù–µ –æ–¥–æ–±—Ä–∏–ª–∏ –æ–±—â–µ–∂–∏—Ç–∏–µ", "–û—Ç–∫–ª–æ–Ω–∏–ª–∏ –∑–∞—è–≤–∫—É –Ω–∞ –æ–±—â–∞–≥—É"
            ],
            "–ö–∞–∫ –ø–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É –Ω–∞ –æ–±—â–µ–∂–∏—Ç–∏–µ?": [
                "–ö–∞–∫ –ø–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É?", "–ß–µ –Ω—É–∂–Ω–æ –¥–ª—è –∑–∞—è–≤–∫–∏?", "–ö–∞–∫ –ø–æ–¥–∞–≤–∞—Ç—å –Ω–∞ –æ–±—â–∞–≥—É?",
                "–ö—É–¥–∞ –ø–æ–¥–∞–≤–∞—Ç—å –∑–∞—è–≤–∫—É?", "–ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –∑–∞—è–≤–∫—É –Ω–∞ –æ–±—â–µ–∂–∏—Ç–∏–µ?",
                "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–∫–∏", "–ö–∞–∫ –ø–æ–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –æ–±—â–∞–≥—É?",
                "–ó–∞—è–≤–∫–∞ –Ω–∞ –æ–±—â–µ–∂–∏—Ç–∏–µ –∫–∞–∫?", "–ö–∞–∫ –ø–æ–ø–∞—Å—Ç—å –≤ –æ–±—â–∞–≥—É?"
            ],
            "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –æ–±—â–µ–∂–∏—Ç–∏—è?": [
                "–ß–µ –Ω—É–∂–Ω–æ –¥–ª—è –æ–±—â–∞–≥–∏?", "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?", "–ß—Ç–æ –≤–∑—è—Ç—å –¥–ª—è –æ–±—â–µ–∂–∏—Ç–∏—è?",
                "–ö–∞–∫–∏–µ —Å–ø—Ä–∞–≤–∫–∏ –Ω—É–∂–Ω—ã?", "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—â–∞–≥–∏", "–ß—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è –≤ –æ–±—â–∞–≥—É?",
                "–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—â–µ–∂–∏—Ç–∏—è", "–ë—É–º–∞–≥–∏ –¥–ª—è –æ–±—â–∞–≥–∏ –∫–∞–∫–∏–µ?"
            ],
            "–ö–∞–∫–∏–µ –æ–±—â–µ–∂–∏—Ç–∏—è –µ—Å—Ç—å –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ?": [
                "–ö–∞–∫–∏–µ –æ–±—â–∞–≥–∏ –µ—Å—Ç—å?", "–°–ø–∏—Å–æ–∫ –æ–±—â–∞–≥", "–í—Å–µ –æ–±—â–µ–∂–∏—Ç–∏—è",
                "–°–∫–æ–ª—å–∫–æ –æ–±—â–∞–≥?", "–ö–∞–∫–∏–µ –µ—Å—Ç—å –æ–±—â–µ–∂–∏—Ç–∏—è?", "–û–±—â–∞–≥–∏ –∫–∞–∫–∏–µ –µ—Å—Ç—å?",
                "–í—Å–µ –æ–±—â–∞–≥–∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞", "–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±—â–µ–∂–∏—Ç–∏–π"
            ]
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        for original_question, variants in slang_variants.items():
            # –ù–∞—Ö–æ–¥–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            original_answer = None
            for item in base_data:
                if item['instruction'] == original_question:
                    original_answer = item['output']
                    break

            if original_answer:
                for variant in variants:
                    enhanced_data.append({
                        "instruction": variant,
                        "output": original_answer
                    })

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å—Ç–æ–ø-—Ñ—Ä–∞–∑ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        unknown_questions = [
            "–ü—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞?",
            "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞?",
            "–ì–¥–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞?",
            "–ö–æ–≥–¥–∞ –∫–∞–Ω–∏–∫—É–ª—ã?",
            "–ß—Ç–æ –Ω–∞ –æ–±–µ–¥?",
            "–ö–∞–∫ –¥–µ–ª–∞?",
            "–ß–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç?",
            "–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç",
            "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏?"
        ]

        stop_answers = [
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.",
            "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —É –º–µ–Ω—è –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.",
            "–≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –º–æ–∏—Ö –∑–Ω–∞–Ω–∏–π.",
            "–ü–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –ª—É—á—à–µ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏."
        ]

        for i, question in enumerate(unknown_questions):
            enhanced_data.append({
                "instruction": question,
                "output": stop_answers[i % len(stop_answers)]
            })

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(enhanced_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return enhanced_data

    def prepare_training_data(self, enhanced_data):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        training_texts = []

        for item in enhanced_data:
            # –§–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            text = f"–í–æ–ø—Ä–æ—Å: {item['instruction']}\n–û—Ç–≤–µ—Ç: {item['output']}<|endoftext|>"
            training_texts.append({"text": text})

        return Dataset.from_list(training_texts)

    def setup_model_and_tokenizer(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        print("üß† –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —É–º–Ω—É—é –º–æ–¥–µ–ª—å...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float32,
            device_map=None
        )

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["c_attn", "c_proj"],
            bias="none",
            inference_mode=False,
        )

        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()

        print("‚úÖ –£–º–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")

    def tokenize_function(self, examples):
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã"""
        result = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=256,
            return_tensors=None
        )

        result["labels"] = result["input_ids"].copy()
        return result

    def train(self):
        """–û–±—É—á–∞–µ—Ç —É–º–Ω—É—é –º–æ–¥–µ–ª—å"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —É–º–Ω–æ–π –º–æ–¥–µ–ª–∏...")

        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        enhanced_data = self.create_enhanced_dataset()

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        dataset = self.prepare_training_data(enhanced_data)

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        self.setup_model_and_tokenizer()

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        tokenized_dataset = dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            overwrite_output_dir=True,
            num_train_epochs=12,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            per_device_train_batch_size=1,
            gradient_accumulation_steps=8,
            warmup_steps=150,
            logging_steps=10,
            save_steps=200,
            evaluation_strategy="no",
            save_total_limit=3,
            prediction_loss_only=True,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            learning_rate=3e-4,
            weight_decay=0.01,
            fp16=False,
            gradient_checkpointing=False,
            dataloader_num_workers=0,
            report_to=None,
        )

        # –ö–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            return_tensors="pt",
        )

        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )

        # –û–±—É—á–∞–µ–º!
        print("üéØ –ó–∞–ø—É—Å–∫–∞–µ–º —É–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        trainer.train()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–º–Ω—É—é –º–æ–¥–µ–ª—å...")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)

        print("üéâ –£–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –£–º–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")


def main():
    print("üß† –û–ë–£–ß–ï–ù–ò–ï –£–ú–ù–û–ô –ú–û–î–ï–õ–ò –° –†–ê–ó–ì–û–í–û–†–ù–´–ú –Ø–ó–´–ö–û–ú")
    print("=" * 70)

    trainer = SmartTrainer()
    trainer.train()

    print("\n‚úÖ –£–ú–ù–ê–Ø –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê!")


if __name__ == "__main__":
    main()
