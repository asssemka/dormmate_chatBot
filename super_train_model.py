import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import json
import os


class FixedSuperTrainer:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self.model_name = "ai-forever/rugpt3small_based_on_gpt2"  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ —Ä–∞–±–æ—á–µ–π
        self.output_dir = "./fixed_super_model"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def load_dataset(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")

        data = []
        with open("dataset.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    item = json.loads(line.strip())
                    # –ü—Ä–æ—Å—Ç–æ–π –∏ —á–µ—Ç–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
                    text = f"–í–æ–ø—Ä–æ—Å: {item['instruction']}\n–û—Ç–≤–µ—Ç: {item['output']}<|endoftext|>"
                    data.append({"text": text})

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return Dataset.from_list(data)

    def setup_model_and_tokenizer(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ"""
        print("üß† –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # –ü—Ä–æ—Å—Ç–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float32,
            device_map=None
        )

        # –ü—Ä–æ—Å—Ç–∞—è LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["c_attn", "c_proj"],
            bias="none",
            inference_mode=False,
        )

        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()

        print("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")

    def tokenize_function(self, examples):
        """–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
        result = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,  # –£–±–∏—Ä–∞–µ–º padding –∑–¥–µ—Å—å
            max_length=256,
            return_tensors=None  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏, –Ω–µ —Ç–µ–Ω–∑–æ—Ä—ã
        )

        # –ö–æ–ø–∏—Ä—É–µ–º input_ids –≤ labels –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        result["labels"] = result["input_ids"].copy()

        return result

    def train(self):
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        dataset = self.load_dataset()

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        self.setup_model_and_tokenizer()

        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        tokenized_dataset = dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )

        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            overwrite_output_dir=True,
            num_train_epochs=10,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            logging_steps=10,
            save_steps=200,
            evaluation_strategy="no",
            save_total_limit=3,
            prediction_loss_only=True,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            learning_rate=5e-4,
            weight_decay=0.01,
            fp16=False,
            gradient_checkpointing=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            dataloader_num_workers=0,
            report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        )

        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            return_tensors="pt",
        )

        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )

        # –û–±—É—á–∞–µ–º!
        print("üéØ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        trainer.train()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)

        print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")


def main():
    print("üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –°–£–ü–ï–† –ú–û–î–ï–õ–ò")
    print("=" * 60)

    trainer = FixedSuperTrainer()
    trainer.train()

    print("\n‚úÖ –ì–û–¢–û–í–û!")


if __name__ == "__main__":
    main()
