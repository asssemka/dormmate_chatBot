import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import json
import os

class RealStudentChatbotTrainer:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å
        self.model_name = "ai-forever/rugpt3small_based_on_gpt2"
        self.output_dir = "./real_student_model"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def load_dataset(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –†–ï–ê–õ–¨–ù–´–ô –¥–∞—Ç–∞—Å–µ—Ç...")
        
        data = []
        with open("real_dataset.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    item = json.loads(line.strip())
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                    text = f"–°—Ç—É–¥–µ–Ω—Ç: {item['question']}\n–ü–æ–º–æ—â–Ω–∏–∫: {item['answer']}<|endoftext|>"
                    data.append({"text": text})
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        return Dataset.from_list(data)
    
    def setup_model_and_tokenizer(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
        print("üß† –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float32,
            device_map=None
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["c_attn", "c_proj"]
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
    
    def tokenize_function(self, examples):
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=300,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            return_tensors="pt"
        )
    
    def train(self):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        dataset = self.load_dataset()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        self.setup_model_and_tokenizer()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        tokenized_dataset = dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            overwrite_output_dir=True,
            num_train_epochs=8,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            per_device_train_batch_size=1,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            logging_steps=10,
            save_steps=200,
            evaluation_strategy="no",
            save_total_limit=3,
            prediction_loss_only=True,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            learning_rate=5e-4,
            fp16=False,
        )
        
        # –ö–æ–ª–ª–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # –û–±—É—á–∞–µ–º!
        print("üéØ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å...")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.output_dir)
        
        print("üéâ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}")

def main():
    print("üéì –û–ë–£–ß–ï–ù–ò–ï –ù–ê –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –£–ù–ò–í–ï–†–°–ò–¢–ï–¢–ê")
    print("=" * 60)
    
    trainer = RealStudentChatbotTrainer()
    trainer.train()
    
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä —Å —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é")

if __name__ == "__main__":
    main()
