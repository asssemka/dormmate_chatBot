from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import logging
from typing import Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="ü§ñ AI –°—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–π –ß–∞—Ç–ë–æ—Ç", version="2.0.0")

class ChatRequest(BaseModel):
    question: str
    user_id: Optional[str] = None
    max_tokens: int = 100

class ChatResponse(BaseModel):
    answer: str
    model_type: str
    generation_time: float

class AIStudentChatbot:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            model_path = "./trained_student_model"
            base_model_name = "microsoft/DialoGPT-small"
            
            print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {self.device}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
            import os
            if os.path.exists(model_path):
                print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –û–ë–£–ß–ï–ù–ù–£–Æ –º–æ–¥–µ–ª—å...")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
                self.model = PeftModel.from_pretrained(base_model, model_path)
                self.model = self.model.to(self.device)
                
                print("‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                
            else:
                print("‚ö†Ô∏è –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é...")
                
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                self.model = self.model.to(self.device)
                
                print("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model.eval()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise e
    
    def generate_response(self, question: str, max_tokens: int = 100) -> tuple:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        import time
        
        start_time = time.time()
        
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=200
            ).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            if "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:" in response:
                answer = response.split("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")[-1].strip()
            else:
                answer = response.replace(prompt, "").strip()
            
            generation_time = time.time() - start_time
            
            return answer, generation_time
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}", 0

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –±–æ—Ç–∞
chatbot = AIStudentChatbot()

@app.get("/")
def root():
    model_type = "–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å" if "./trained_student_model" else "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"
    return {
        "message": "ü§ñ AI –°—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–π –ß–∞—Ç–ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "model_type": model_type,
        "device": str(chatbot.device),
        "status": "ready"
    }

@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é AI"""
    try:
        logger.info(f"üìù –í–æ–ø—Ä–æ—Å –æ—Ç {request.user_id}: {request.question}")
        
        answer, gen_time = chatbot.generate_response(
            request.question, 
            request.max_tokens
        )
        
        model_type = "trained_model" if "./trained_student_model" else "base_model"
        
        return ChatResponse(
            answer=answer,
            model_type=model_type,
            generation_time=gen_time
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="localhost", port=8000)
