from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import logging
from typing import Optional
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="üè´ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏–π AI –ß–∞—Ç–ë–æ—Ç", version="4.0.0")

class ChatRequest(BaseModel):
    question: str
    user_id: Optional[str] = None
    max_tokens: int = 150

class ChatResponse(BaseModel):
    answer: str
    confidence: float
    model_info: str
    generation_time: float

class UniversityAIChatbot:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            model_path = "./real_student_model"
            base_model_name = "ai-forever/rugpt3small_based_on_gpt2"
            
            print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {self.device}")
            
            import os
            if os.path.exists(model_path):
                print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –û–ë–£–ß–ï–ù–ù–£–Æ –º–æ–¥–µ–ª—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
                self.model = PeftModel.from_pretrained(base_model, model_path)
                self.model = self.model.to(self.device)
                
                print("‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                
            else:
                print("‚ö†Ô∏è –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é...")
                
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                self.model = self.model.to(self.device)
                
                print("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model.eval()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise e
    
    def clean_response(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Ñ—Ä–∞–∑
        sentences = text.split('.')
        unique_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in unique_sentences:
                unique_sentences.append(sentence)
        
        result = '. '.join(unique_sentences)
        if result and not result.endswith('.'):
            result += '.'
        
        return result
    
    def calculate_confidence(self, question: str, answer: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
        confidence = 0.5
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
        if len(answer) > 20:
            confidence += 0.2
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        university_keywords = ['–æ–±—â–µ–∂–∏—Ç–∏–µ', '–¥–µ–∫–∞–Ω–∞—Ç', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '—Å—Ç—É–¥–µ–Ω—Ç', '–∑–∞—è–≤–∫–∞', '–∞–¥—Ä–µ—Å', '–∞–ª–º–∞—Ç—ã']
        question_lower = question.lower()
        answer_lower = answer.lower()
        
        for keyword in university_keywords:
            if keyword in question_lower and keyword in answer_lower:
                confidence += 0.1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∞–¥—Ä–µ—Å–∞
        if '–∞–ª–º–∞—Ç—ã' in answer_lower and ('–º–∫—Ä' in answer_lower or '—Ç–∞—É–≥—É–ª—å' in answer_lower):
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    def generate_response(self, question: str, max_tokens: int = 150) -> tuple:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        import time
        
        start_time = time.time()
        
        try:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
            prompt = f"–°—Ç—É–¥–µ–Ω—Ç: {question}\n–ü–æ–º–æ—â–Ω–∏–∫:"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=200
            ).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.3,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            if "–ü–æ–º–æ—â–Ω–∏–∫:" in response:
                answer = response.split("–ü–æ–º–æ—â–Ω–∏–∫:")[-1].strip()
            else:
                answer = response.replace(prompt, "").strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            answer = self.clean_response(answer)
            
            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
            if not answer or len(answer) < 5:
                answer = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –¥–µ–∫–∞–Ω–∞—Ç –∏–ª–∏ –æ—Ç–¥–µ–ª —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è."
            
            # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = self.calculate_confidence(question, answer)
            
            generation_time = time.time() - start_time
            
            return answer, confidence, generation_time
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", 0.1, 0

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –±–æ—Ç–∞
chatbot = UniversityAIChatbot()

@app.get("/")
def root():
    import os
    model_type = "–û–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö" if os.path.exists("./real_student_model") else "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"
    return {
        "message": "üè´ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏–π AI –ß–∞—Ç–ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "model_info": model_type,
        "device": str(chatbot.device),
        "university": "–ê–ª–º–∞—Ç—ã",
        "status": "ready"
    }

@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤"""
    try:
        logger.info(f"üìù –í–æ–ø—Ä–æ—Å –æ—Ç {request.user_id}: {request.question}")
        
        answer, confidence, gen_time = chatbot.generate_response(
            request.question, 
            request.max_tokens
        )
        
        import os
        model_info = "real_trained_model" if os.path.exists("./real_student_model") else "base_model"
        
        return ChatResponse(
            answer=answer,
            confidence=confidence,
            model_info=model_info,
            generation_time=gen_time
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="localhost", port=8000)
