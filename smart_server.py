from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, ConfigDict
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import logging
from typing import Optional
import time
import re
from smart_database import SmartDatabase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="üß† –£–ú–ù–´–ô AI –ß–∞—Ç–ë–æ—Ç —Å –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º –Ø–∑—ã–∫–æ–º", version="8.0.0")


class ChatRequest(BaseModel):
    question: str
    user_id: Optional[str] = None
    max_tokens: int = 100


class ChatResponse(BaseModel):
    model_config = ConfigDict(protected_namespaces=())

    answer: str
    confidence: float
    model_info: str
    generation_time: float
    source: str  # "database" –∏–ª–∏ "model"


class SmartChatbot:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.database = SmartDatabase()
        self.load_model()

    def load_model(self):
        try:
            model_path = "./smart_model"
            base_model_name = "ai-forever/rugpt3small_based_on_gpt2"
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —É–º–Ω—É—é –º–æ–¥–µ–ª—å —Å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {self.device}")
            import os
            if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "adapter_config.json")):
                print("–ó–∞–≥—Ä—É–∂–∞–µ–º –£–ú–ù–£–Æ –û–ë–£–ß–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨...")
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                self.model = PeftModel.from_pretrained(base_model, model_path)
                self.model = self.model.to(self.device)
                print("–£–ú–ù–ê–Ø –ú–û–î–ï–õ–¨ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            else:
                print("–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é...")
                self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map=None
                )
                self.model = self.model.to(self.device)
                print("–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            self.model.eval()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            logger.info(" –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Ç–æ–ª—å–∫–æ —Å —É–º–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö")

    def clean_response(self, text: str) -> str:
        artifacts = [
            r'<\|.*?\|>', r'<.*?>', r'\|.*?\|',
            r'<endoftext>', r'<startoftext>', r'<pad>',
            r'endof.*?', r'enabled.*?', r'Beta.*?',
            r'Factory.*?', r'Container.*?', r'\[.*?\]'
        ]

        for artifact in artifacts:
            text = re.sub(artifact, '', text, flags=re.IGNORECASE)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^\w\s.,!?:;‚Ññ()-]', '', text)
        text = re.sub(r'\s+', ' ', text)

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –æ–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–µ
        sentences = text.split('.')
        if sentences and len(sentences[0].strip()) > 10:
            result = sentences[0].strip() + '.'
        else:
            result = text.strip()

        return result

    def generate_model_response(self, question: str, max_tokens: int = 100) -> tuple:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏"""
        if self.model is None or self.tokenizer is None:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.", 0.0, 0.0

        start_time = time.time()

        try:
            # –ü—Ä–æ–º–ø—Ç
            prompt = f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=200
            ).to(self.device)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.3,
                    no_repeat_ngram_size=3,
                    early_stopping=True
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            if "–û—Ç–≤–µ—Ç:" in response:
                answer = response.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            else:
                answer = response.replace(prompt, "").strip()

            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            answer = self.clean_response(answer)

            generation_time = time.time() - start_time

            return answer, 0.6, generation_time

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞.", 0.1, 0.0

    def get_smart_answer(self, question: str, max_tokens: int = 100) -> tuple:
        """–ü–æ–ª—É—á–∞–µ—Ç —É–º–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏"""
        start_time = time.time()

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —É–º–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        db_answer, db_confidence = self.database.get_smart_answer(question)

        # –ï—Å–ª–∏ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if db_confidence >= 0.7:
            generation_time = time.time() - start_time
            return db_answer, db_confidence, generation_time, "database"

        # –ï—Å–ª–∏ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–∞–ª–∞ —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
        if db_confidence <= 0.2 and self.model is not None:
            model_answer, model_confidence, model_time = self.generate_model_response(question, max_tokens)

            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–∞–ª–∞ –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç
            if model_confidence > db_confidence and len(model_answer) > 10:
                return model_answer, model_confidence, model_time, "model"

        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–≤–∫–ª—é—á–∞—è —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã)
        generation_time = time.time() - start_time
        return db_answer, db_confidence, generation_time, "database"


# –°–æ–∑–¥–∞–µ–º —É–º–Ω–æ–≥–æ –±–æ—Ç–∞
chatbot = SmartChatbot()


@app.get("/")
def root():
    import os
    model_type = "–£–ú–ù–ê–Ø –ú–û–î–ï–õ–¨" if os.path.exists("./smart_model") else "–£–º–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö + –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"
    return {
        "message": "üß† –£–ú–ù–´–ô AI –ß–∞—Ç–ë–æ—Ç —Å —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º —è–∑—ã–∫–æ–º —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "model_info": model_type,
        "device": str(chatbot.device),
        "university": "–ê–ª–º–∞—Ç—ã",
        "database_size": len(chatbot.database.qa_pairs),
        "features": ["—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —è–∑—ã–∫", "—Å—Ç–æ–ø-—Ñ—Ä–∞–∑—ã", "—É–º–Ω—ã–π –ø–æ–∏—Å–∫"],
        "status": "ready"
    }


@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞"""
    try:
        logger.info(f"üìù –£–º–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ—Ç {request.user_id}: {request.question}")

        answer, confidence, gen_time, source = chatbot.get_smart_answer(
            request.question,
            request.max_tokens
        )

        import os
        model_info = "smart_hybrid_model" if os.path.exists("./smart_model") else "smart_database"

        return ChatResponse(
            answer=answer,
            confidence=confidence,
            model_info=model_info,
            generation_time=gen_time,
            source=source
        )

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
